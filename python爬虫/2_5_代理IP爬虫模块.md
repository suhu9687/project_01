## 8 爬虫模块

### 爬虫模块的设计思路
- 实现一个通用爬虫, 原因代理IP网站的页面结构几乎都是Table, 页面结构类似
- 具体爬虫: 通过继承通用爬虫实现具体网站的抓取, 只需要指定爬取的URL列表, 分组的XPATH和组内XPATH就可以了. 
- 爬虫运行模块: 通过配置文件控制启用哪些爬虫, 增加扩展性; 如果将来我们遇到返回json格式的代理网站, 写一个爬虫配置下就好了.

### 实现通用爬虫
- `目标`: 实现一个可以通过指定不同URL列表, 分组的XPATH和详情的XPATH, 从不同页面上提取数据; 
- `步骤`:
1. 创建spiders的包, 创建base_spider.py文件
2. 定义一个类, 继承object
3. 提供三个类成员变量:
    - urls: 代理IP网址的URL的列表
    - group_xpath: 分组XPATH, 获取包含代理IP信息标签列表的XPATH
    - detail_xpath: 组内XPATH, 获取代理IP详情的信息XPATH, 格式为: {'ip':'xx', 'port':'xx', 'area':'xx'}
4. 提供初始方法, 传入爬虫URL列表, 分组XPATH, 详情(组内)XPATH
5. 对外提供一个获取代理IP的方法 
    - 遍历URL列表, 获取URL
    - 根据发送请求, 获取页面数据
      - 实现随机的User-Agent请求头
    - 解析页面, 提取数据
    - 把数据返回

- `代码`
    - 通用爬虫类
    ```py 
    #!/usr/bin/python3
    # -*- coding: utf-8 -*-
    import requests
    from lxml import etree
    from domain import Proxy
    from utils.http import get_request_header

    class BaseSpider(object):

        urls = [] #  代理IP网址的URL的列表
        group_xpath='' # 分组XPATH, 获取包含代理IP信息标签列表的XPATH
        detail_xpath = {} # 组内XPATH, 获取代理IP详情的信息XPATH

        def __init__(self, urls=[], group_xpath=None, detail_xpath={}):

            if urls: # 如果urls中有数据
                self.urls = urls
            if group_xpath: # 如果group_xpath中有数据
                self.group_xpath = group_xpath
            if detail_xpath: # 如果detail_xpath中有数据
                self.detail_xpath = detail_xpath

        def get_page_from_url(self, url):
            response = requests.get(url, headers=get_request_header())
            return response.content

        def get_first(self, lis):
            return lis[0].strip() if len(lis) != 0 else ''

        def get_proxyes_from_page(self, page):
            """解析页面数据"""
            element = etree.HTML(page)
            trs = element.xpath(self.group_xpath)
            # print(len(trs))
            for tr in trs:
                ip = self.get_first(tr.xpath(self.detail_xpath['ip']))
                port = self.get_first(tr.xpath(self.detail_xpath['port']))
                area = self.get_first(tr.xpath(self.detail_xpath['area']))
                proxy = Proxy(ip, port, area=area)
                # 返回代理IP
                yield proxy

        def get_proxies(self):
            """获取代理IP信息"""
            # - 遍历URL列表, 获取URL
            for url in self.urls:
                # - 根据发送请求, 获取页面数据
                page = self.get_page_from_url(url)
                # - 解析页面, 提取数据
                proxies = self.get_proxyes_from_page(page)
                # - 把数据返回
                yield from proxies

    if __name__ == '__main__':
        config = {
            'urls':['https://www.xicidaili.com/nn/1'.format(i) for i in range(1, 2)],
            'group_xpath': '//*[@id="ip_list"]/tr[position()>1]',
            'detail_xpath': {'ip':'./td[2]/text()', 'port':'./td[3]/text()', 'area':'./td[4]/a/text()'},
        }
        # 创建通用代理对象
        base_spider = BaseSpider(**config)
        for proxy in base_spider.get_proxies():
            print(proxy)
    ```


### 实现具体的爬虫类
- `目标`: 通过继承通用爬虫, 实现多个爬虫, 分布从各大免费代理IP网站上抓取代理IP
- `步骤`:
    0. 创建proxy_spiders.py文件
    1. 实现`西刺代理`爬虫: `http://www.xicidaili.com/nn/1`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
  
    2. 实现`ip3366代理`爬虫: `http://www.ip3366.net/free/?stype=1&page=1`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
    3. 实现`ip嗨代理`爬虫:   `http://www.iphai.com/free/ng`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
    4. 实现`proxylistplus代理`爬虫: `https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
    5. 实现`66ip`爬虫: `http://www.66ip.cn/1.html`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
        - 由于66ip网页进行js + cookie反爬, 需要重写父类的`get_page_from_url`方法

- `完整代码`

```py
import requests
import re
import js2py

from spiders.base_spider import BaseSpider
from utils.http import get_request_header
from domain import Proxy

"""
1. 实现`西刺代理`爬虫: `http://www.xicidaili.com/nn/1`
    - 定义一个类,继承通用爬虫类(BasicSpider)
    - 提供urls, group_xpath 和 detail_xpath
"""

class XiciSpider(BaseSpider):
    urls = ['http://www.xicidaili.com/nn/{}'.format(i) for i in range(1, 10)]
    group_xpath = '//*[@id="ip_list"]/tr[position()>1]'
    detail_xpath = {'ip': './td[2]/text()', 'port': './td[3]/text()', 'area': './td[4]/a/text()'}



"""
2. 实现`ip3366代理`爬虫: `http://www.ip3366.net/free/?stype=1&page=1`
        - 定义一个类,继承通用爬虫类(BasicSpider)
        - 提供urls, group_xpath 和 detail_xpath
"""
class Ip3366Spider(BaseSpider):
    urls = ['http://www.ip3366.net/free/?stype={}&page={}'.format(i, j) for j in range(1, 10) for i in range(1, 4, 2)]
    group_xpath = '//*[@id="list"]/table/tbody/tr'
    detail_xpath = {'ip':'./td[1]/text()', 'port':'./td[2]/text()','area':'./td[5]/text()' }

"""
3. 实现`ip嗨代理`爬虫:   `http://www.iphai.com/free/ng`
    - 定义一个类,继承通用爬虫类(BasicSpider)
    - 提供urls, group_xpath 和 detail_xpath
"""
class IphaiSpider(BaseSpider):
    urls = ['http://www.iphai.com/free/ng', 'http://www.iphai.com/free/wg']
    group_xpath = '//table/tr[position()>1]'
    detail_xpath = {'ip':'./td[1]/text()', 'port':'./td[2]/text()', 'area':'./td[5]/text()' }

"""
4. 实现`proxylistplus代理`爬虫: `https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1`
    - 定义一个类,继承通用爬虫类(BasicSpider)
    - 提供urls, group_xpath 和 detail_xpath
"""
class ProxylistplusSpider(BaseSpider):
    urls = ['https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-{}'.format(i) for i in range(1, 7)]
    group_xpath = '//*[@id="page"]/table[2]/tr[position()>2]'
    detail_xpath = {'ip':'./td[2]/text()', 'port':'./td[3]/text()', 'area':'./td[5]/text()'}

"""
5. 实现`66ip`爬虫: `http://www.66ip.cn/1.html`
    - 定义一个类,继承通用爬虫类(BasicSpider)
    - 提供urls, group_xpath 和 detail_xpath
"""
class Ip66Spider(BaseSpider):
    urls = ['http://www.66ip.cn/{}.html'.format(i) for i in range(1, 10)]
    group_xpath = '//*[@id="main"]/div/div[1]/table/tr[position()>1]'
    detail_xpath = {'ip': './td[1]/text()', 'port': './td[2]/text()', 'area': './td[3]/text()'}
    def get_page_from_url(self, url):
        """发送请求, 获取响应的方法"""
        # 获取session对象, session可以记录服务器设置过来的cookie信息
        session = requests.session()
        session.headers = get_request_header()
        respsone = session.get(url)
        # 如果响应码是521
        if respsone.status_code == 521:
            # 通过正则获取, 需要执行的js
            rs = re.findall('window.onload=setTimeout\("(\w+\(\d+\))", \d+\); (function \w+\(\w+\).+?)</script>', respsone.content.decode())
            # 获取js2py的js执行环境
            context = js2py.EvalJs()

            # 把执行执行js, 修改为返回要执行的js
            func = rs[0][1].replace('eval("qo=eval;qo(po);");', 'return po;')
            # 让js执行环境, 加载要执行的js
            context.execute(func)
            # 把函数的执行结果赋值给一个变量
            context.execute( "a={}".format(rs[0][0]))
            # 从变量中取出cookie信息
            cookie = re.findall("document.cookie='(\w+)=(.+?);", context.a)
            # 把从js中提取的cookie信息设置给session
            session.cookies[cookie[0][0]] = cookie[0][1]
            # print(session.cookies)
            respsone = session.get(url)

        return respsone.content.decode('gbk')

if __name__ == '__main__':
    # spider = XiciSpider()
    # spider = Ip3366Spider()
    # spider = IphaiSpider()
    # spider = ProxylistplusSpider()
    spider = Ip66Spider()
    for proxy in spider.get_proxies():
        print(proxy)
```

### 运行爬虫模块
- `目标`: 根据配置文件信息, 加载爬虫, 获取代理IP, 进行检验, 检验后写入到数据库中
- `思路`: 
   - 在`spiders`目录下创建`run_spider.py`文件, 创建RunSpider类
   - 提供一个运行爬虫的`run`方法
    - 根据配置文件信息, 加载爬虫, 把爬虫对象放到列表中
    - 遍历爬虫对象列表, 获取代理, 检测代理(代理IP检测模块), 写入数据库(数据库模块) 
   - 使用异步来执行每一个爬虫任务
   - 每隔一定的时间, 执行一次爬取任务

- `步骤`:
  - 修改 setting.py 增加 代理IP爬虫的配置信息
  ```py
    # 配置代理爬虫列表
    PROXIES_SPIDERS = [
        'spiders.proxy_spiders.Ip66Spider',
        'spiders.proxy_spiders.Ip3366Spider',
        'spiders.proxy_spiders.IphaiSpider',
        'spiders.proxy_spiders.ProxylistplusSpider',
        'spiders.proxy_spiders.XiciSpider',
    ]
  ```
  - 在spiders包下, 创建run_spider.py中创建RunSpider类
  - 实现根据配置文件, 加载爬虫, 把爬虫对象放到列表中的方法
    1. 定义一个列表, 用于存储爬虫对象
    2. 遍历爬虫配置信息, 获取每一个爬虫路径
    3. 根据爬虫路径获取模块名和类名
    4. 使用importlib根据模块名, 导入该模块
    5. 根据类名, 从模块中获取类
    6. 使用类创建对象, 添加到对象列表中
    
    ```py
    def _auto_import_instances(self):
        """根据配置信息, 自动导入爬虫"""
        instances = []
        # 遍历配置的爬虫, 获取爬虫路径
        for path in settings.PROXIES_SPIDERS:
            # 根据路径, 获取模块名 和 类名
            module_name, cls_name = path.rsplit('.', maxsplit=1)
            # 根据模块名导入模块
            module = importlib.import_module(module_name)
            # 根据类名, 从模块中, 获取爬虫类
            cls = getattr(module, cls_name)
            # 创建爬虫对象, 添加到列表中
            instances.append(cls())
        # 返回爬虫对象列表
        return instances
    ```

- 实现run方法, 用于运行整个爬虫
   1. 获取代理爬虫列表
   2. 遍历代理爬虫列表
   3. 遍历爬虫的get_proxies()方法, 获取代理IP
   4. 如果代理IP为None, 继续一次循环
   5. 检查代理, 获取代理协议类型, 匿名程度, 和速度
   6. 如果代理速度不为-1, 就是说明该代理可用, 保存到数据库中
   7. 处理下异常, 防止一个爬虫内部错误, 其他爬虫都运行不了
   ```py
    class RunSpider(object):
      def __init__(self):
         self.proxy_pool = MongoPool()

      def run(self):
        """启动爬虫"""
        # 获取代理爬虫
        spiders = self._auto_import_instances()
        # 执行爬虫获取代理
        for spider in spiders:
            try:
                for proxy in spider.get_proxies():
                    if proxy is None:
                        # 如果是None继续一个
                        continue
                    # 检查代理, 获取代理协议类型, 匿名程度, 和速度
                    proxy = check_proxy(proxy)
                    # 如果代理速度不为-1, 就是说明该代理可用
                    if proxy.speed != -1:
                        # 保存该代理到数据库中
                        self.proxy_pool.save(proxy)
            except Exception as e:
                logger.exception(e)
                logger.exception("爬虫{} 出现错误".format(spider))
   ```

- 使用协程池异步来运行每一个爬虫, 以提高爬取的速度
  1. 实现__init__方法, 创建协程池
  2. 把执行处理每一个爬虫的代码抽取一个方法
  3. 使用异步调用这个方法

  ```py
  class RunSpider(object):
    def __init__(self):
        self.proxy_pool = MongoPool()
        self.pool = Pool()
    
    ...

    def run(self):
        """启动爬虫"""
        # 获取代理爬虫
        spiders = self._auto_import_instances()
        # 执行爬虫获取代理
        for spider in spiders:
           # 使用协程异步调用该方法,提高爬取的效率
           self.pool.apply_async(self.__run_one_spider, args=(spider, ))

        # 等待所有爬虫任务执行完毕
        self.pool.join()

    def __run_one_spider(self, spider):
        try:
            for proxy in spider.get_proxies():
                if proxy is None:
                    # 如果是None继续一个
                    continue
                # 检查代理, 获取代理协议类型, 匿名程度, 和速度
                proxy = check_proxy(proxy)
                # 如果代理速度不为-1, 就是说明该代理可用
                if proxy.speed != -1:
                    # 保存该代理到数据库中
                    self.proxy_pool.save(proxy)
        except Exception as e:
            logger.exception(e)
            logger.exception("爬虫{} 出现错误".format(spider))
  ```

- 每隔一定的时间, 执行一次爬取任务
    1. 修改 `setting.py` 文件, 爬虫间隔时间的配置
    ```py
        # 抓取IP的时间间隔, 单位小时
        SPIDER_INTERVAL = 2
    ```
   2. 安装 `schedule`: `pip3 install schedule` 
   3. 在 `RunSpider` 中提供start的类方法, 用于启动爬虫的运行, 每间隔指定时间, 重新运行一次.
   ```py
       @classmethod
    def start(cls):
        # 创建本类对象
        run_spider = RunSpider()
        run_spider.run()

        # 每隔 SPIDER_INTERVAL 小时检查下代理是否可用
        schedule.every(settings.SPIDER_INTERVAL).hours.do(run_spider.run())
        while True:
            schedule.run_pending()
            time.sleep(1)
   ```

- 爬虫运行模块完整代码

```py
#!/usr/bin/python3
# -*- coding: utf-8 -*-
from gevent import monkey
monkey.patch_all()
from gevent.pool import Pool
import importlib
import schedule
import time

import settings
from validator.httpbin_validator import check_proxy
from db.mongo_pool import MongoPool
from utils.log import logger

class RunSpider(object):
    def __init__(self):
        self.pool = Pool()
        self.proxy_pool = MongoPool()

    def _auto_import_instances(self):
        """根据配置信息, 自动导入爬虫"""
        instances = []
        # 遍历配置的爬虫, 获取爬虫路径
        for path in settings.PROXIES_SPIDERS:
            # 根据路径, 获取模块名 和 类名
            module_name, cls_name = path.rsplit('.', maxsplit=1)
            # 根据模块名导入模块
            module = importlib.import_module(module_name)
            # 根据类名, 从模块中, 获取爬虫类
            cls = getattr(module, cls_name)
            # 创建爬虫对象, 添加到列表中
            instances.append(cls())

        # 返回爬虫对象列表
        return instances

    def run(self):
        """启动爬虫"""
        # 获取代理爬虫
        spiders = self._auto_import_instances()
        # 执行爬虫获取代理
        for spider in spiders:
           # 使用协程异步调用该方法,提高爬取的效率
           self.pool.apply_async(self.__run_one_spider, args=(spider, ))

        # 等待所有爬虫任务执行完毕
        self.pool.join()

    def __run_one_spider(self, spider):
        try:
            for proxy in spider.get_proxies():
                if proxy is None:
                    # 如果是None继续一个
                    continue
                # 检查代理, 获取代理协议类型, 匿名程度, 和速度
                proxy = check_proxy(proxy)
                # 如果代理速度不为-1, 就是说明该代理可用
                if proxy.speed != -1:
                    # 保存该代理到数据库中
                    self.proxy_pool.save(proxy)
        except Exception as e:
            logger.exception(e)
            logger.exception("爬虫{} 出现错误".format(spider))

    @classmethod
    def start(cls):
        # 创建本类对象
        run_spider = RunSpider()
        run_spider.run()

        # 每隔 SPIDER_INTERVAL 小时检查下代理是否可用
        schedule.every(settings.SPIDER_INTERVAL).hours.do(run_spider.run())
        while True:
            schedule.run_pending()
            time.sleep(1)


if __name__ == '__main__':
    RunSpider.start()
```

  
